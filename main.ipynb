{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning with PyTorch: Neural Style Transfer\n",
    "# set Google Colab runtime\n",
    "\n",
    "# !pip install torch torchvision\n",
    "# !git clone https://github.com/parth1620/Project-NST.git\n",
    "\n",
    "# loading VSG-19 pretrained model using model subpackage in PyTorch\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "vgg = models.vgg19(pretrained = True)\n",
    "\n",
    "# prints features and classifiers and their layers\n",
    "# for this project, we don't use the classifiers, as we extract the content and style from the layers in the features\n",
    "print(vgg)\n",
    "\n",
    "vgg = vgg.features\n",
    "# see only the features component\n",
    "print(vgg)\n",
    "\n",
    "# parse the gradient, use the pretrained weight without updating it\n",
    "# freeze the model so no gradient computation occurs in the training loop\n",
    "for parameters in vgg.parameters():\n",
    "  parameters.requires_grad_(False)\n",
    "\n",
    "# check and move variables and model to GPU if available, otherwise to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print to check what device was set to\n",
    "print(device)\n",
    "vgg.to(device)\n",
    "\n",
    "# preprocess image\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# create preprocess function\n",
    "# used to resize the image if it is larger than max_size\n",
    "def preprocess(img_path, max_size = 500):\n",
    "  image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "  if max(image.size) > max_size:\n",
    "    size = max_size\n",
    "  else:\n",
    "    size = max(image.size)\n",
    "\n",
    "  # all pre-trained models expect input images normalized in the same way\n",
    "  # loaded in to a range of [0, 1] and then normalized using mean and std values\n",
    "  # mean and std values copied from the torchvision.models manual documentation\n",
    "  img_transforms = T.Compose([\n",
    "                  T.Resize(size),\n",
    "                  T.ToTensor(),\n",
    "                  T.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                              std = [0.229, 0.224, 0.225])\n",
    "  ])\n",
    "  image = img_transforms(image)\n",
    "\n",
    "  # if image in shape of (channel, height, width) -> (batch size, channel, height, width)\n",
    "  image = image.unsqueeze(0)\n",
    "\n",
    "  return image\n",
    "\n",
    "# pass content and style image to preprocess function\n",
    "content_p = preprocess('/content/Project-NST/content11.jpg')\n",
    "style_p = preprocess('/content/Project-NST/style12.jpg')\n",
    "\n",
    "# pass to GPU\n",
    "content_p = content_p.to(device)\n",
    "style_p = style_p.to(device)\n",
    "\n",
    "# print the shape\n",
    "print(\"Content Shape:\", content_p.shape)\n",
    "print(\"Style Shape:\", style_p.shape)\n",
    "\n",
    "# deprocess image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# deprocess the image to plot the image\n",
    "# opposide of the preprocess\n",
    "\n",
    "def deprocess(tensor):\n",
    "  # to plot content and style image, we pass content tensor and style tensor to cpu\n",
    "  image = tensor.to('cpu').clone()\n",
    "  image = image.numpy()\n",
    "\n",
    "  # (batch size, channel, height, width) -> (channel, height, width)\n",
    "  image = image.squeeze(0)\n",
    "\n",
    "  # (channel, height, width) -> (height, width, channel)\n",
    "  image = image.transpose(1, 2, 0)\n",
    "\n",
    "  # denormalize image with std value and mean value\n",
    "  image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "\n",
    "  image = image.clip(0,1)\n",
    "\n",
    "  return image\n",
    "\n",
    "# deprocess content and style tensor\n",
    "content_d = deprocess(content_p)\n",
    "style_d = deprocess(style_p)\n",
    "\n",
    "# print shape\n",
    "print(\"Deprocess Content Shape:\", content_d.shape)\n",
    "print(\"Deprocess Style Shape:\", style_d.shape)\n",
    "\n",
    "# plot the images using matplotlib\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n",
    "\n",
    "ax1.imshow(content_d)\n",
    "ax2.imshow(style_d)\n",
    "\n",
    "# get content, style features and create gram matrix\n",
    "# extract style and content features from the vgg model that we have previously loaded and defined\n",
    "def get_features(image, model):\n",
    "  # map names\n",
    "  layers = {\n",
    "      '0' : 'conv1_1', #style feature\n",
    "      '5' : 'conv2_1', #style feature\n",
    "      '10' : 'conv3_1', #style feature\n",
    "      '19' : 'conv4_1', #style feature\n",
    "      '21' : 'conv4_2', # content feature\n",
    "      '28' : 'conv5_1' # style feature\n",
    "  }\n",
    "\n",
    "  x = image\n",
    "\n",
    "  # store content and style features\n",
    "  Features = {}\n",
    "\n",
    "  for name, layer in model._modules.items():\n",
    "    \n",
    "    # first layer will be 0 layer\n",
    "    # output is the input for the next layer\n",
    "    x = layer(x)\n",
    "\n",
    "    if name in layers:\n",
    "      Features[layers[name]] = x\n",
    "  \n",
    "  return Features\n",
    "\n",
    "# load content and style features\n",
    "content_f = get_features(content_p, vgg)\n",
    "style_f = get_features(style_p, vgg)\n",
    "\n",
    "# create the gram matrix function\n",
    "# gram matrix is the corelation between the filters\n",
    "# the (channel, height, width) gets unrolled into H X W by C\n",
    "# take the unrolled matrix and do matrix multiplication\n",
    "# H X W by C multiped by C by H X W = gram matrix\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "  # unroll the tensor into a matrix\n",
    "  b, c, h, w = tensor.size()\n",
    "  tensor = tensor.view(c, h*w)\n",
    "  gram = torch.mm(tensor, tensor.t())\n",
    "\n",
    "  return gram\n",
    "\n",
    "# find style features gram matrix of every matrix in style Features dictionary\n",
    "style_grams = {layer : gram_matrix(style_f[layer]) for layer in style_f}\n",
    "\n",
    "# create style and content loss function to apply for the optimization of the target image\n",
    "def content_loss(target_conv4_2, content_conv4_2):\n",
    "  return torch.mean((target_conv4_2 - content_conv4_2)**2)\n",
    "\n",
    "style_weights = {\n",
    "    'conv1_1' : 1.0,\n",
    "    'conv2_1' : 0.75,\n",
    "    'conv3_1' : 0.2,\n",
    "    'conv4_1' : 0.2,\n",
    "    'conv5_1' : 0.2\n",
    "}\n",
    "\n",
    "def style_loss(style_weights, target_features, style_grams):\n",
    "  # loss calculated between the style weights and style grams\n",
    "  loss = 0\n",
    "\n",
    "  for layer in style_weights:\n",
    "    target_f = target_features[layer]\n",
    "    target_gram = gram_matrix(target_f)\n",
    "\n",
    "    style_gram = style_grams[layer]\n",
    "\n",
    "    b, c, h, w = target_f.shape\n",
    "    \n",
    "    layer_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "\n",
    "    loss += layer_loss / (c*h*w)\n",
    "\n",
    "  return loss\n",
    "\n",
    "# initialize target image (the image we want to generate)\n",
    "# can initialize with random noise or content image\n",
    "# we are using the content image\n",
    "target = content_p.clone().requires_grad_(True).to(device)\n",
    "target_f = get_features(target, vgg)\n",
    "print(\"Content Loss:\", content_loss(target_f['conv4_2'], content_f['conv4_2']))\n",
    "print(\"Style Loss:\", style_loss(style_weights, target_f, style_grams))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
